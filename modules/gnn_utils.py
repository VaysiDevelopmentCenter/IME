# modules/gnn_utils.py
import torch
from torch_geometric.data import Data # type: ignore # Conditional import based on availability for type checker

try:
    from .engine import NetworkGraph, ArchitecturalNode, ArchitecturalEdge
except ImportError:
    # Fallback for simpler testing or if engine.py is in PYTHONPATH directly
    from engine import NetworkGraph, ArchitecturalNode, ArchitecturalEdge
    print("Warning: gnn_utils.py using fallback imports for engine components.")


# Canonical list of node types for one-hot encoding.
# The order determines the index in the one-hot vector.
# This list should be comprehensive for all node types generated by various parsers/graph builders.
ALL_NODE_TYPES = sorted(list(set([ # Use set to avoid duplicates then sort for consistency
    "generic",
    # From assembly_graph_demo.py & graph_utils.py
    "instruction", "label",
    # From ast_to_graph_converter.py
    "Module", "FunctionDef", "AsyncFunctionDef", "ClassDef",
    "IfStatement", "ForLoop", "AsyncForLoop", "WhileLoop",
    "TryBlock", "ExceptionHandler",
    "AssignmentStatement", "ExpressionCallStatement", "ReturnStatement",
    "StringExpressionStatement", "GenericExpressionStatement"
])))

NODE_TYPE_TO_INDEX = {name: i for i, name in enumerate(ALL_NODE_TYPES)}
NODE_FEATURE_DIM = len(ALL_NODE_TYPES) # This will be the size of the one-hot vector


def network_graph_to_pyg_data(graph: NetworkGraph) -> Data | None:
    """
    Converts an instance of our NetworkGraph to a PyTorch Geometric Data object.
    Node features (x) are one-hot encoded based on `ArchitecturalNode.node_type`.
    Edges are represented by `edge_index`.
    """
    if not graph.nodes:
        # Return an empty Data object, PyG can handle this.
        # num_nodes=0 is important for some PyG operations.
        return Data(x=torch.empty(0, NODE_FEATURE_DIM, dtype=torch.float32),
                    edge_index=torch.empty(2, 0, dtype=torch.long),
                    num_nodes=0)

    # Consistent ordering of nodes for mapping ID to integer index
    # Sort node IDs to ensure that the mapping is deterministic
    node_ids_sorted = sorted(list(graph.nodes.keys()))
    node_id_to_idx = {node_id: i for i, node_id in enumerate(node_ids_sorted)}

    # Node features (x) - One-hot encoding of node_type
    x = torch.zeros(len(node_ids_sorted), NODE_FEATURE_DIM, dtype=torch.float32)
    for i, node_id_str in enumerate(node_ids_sorted):
        node = graph.nodes[node_id_str]

        # Normalize some AST types for feature consistency if needed
        current_node_type = node.node_type
        if current_node_type == "AsyncFunctionDef": current_node_type = "FunctionDef"
        if current_node_type == "AsyncForLoop": current_node_type = "ForLoop"

        type_idx = NODE_TYPE_TO_INDEX.get(current_node_type)
        if type_idx is not None:
            x[i, type_idx] = 1.0
        else:
            # Fallback for unknown node types: use 'generic' or a zero vector
            # print(f"Warning: Unknown node_type '{node.node_type}' for node '{node.id}'. Using 'generic' feature.")
            generic_idx = NODE_TYPE_TO_INDEX.get("generic")
            if generic_idx is not None: # Should always exist if "generic" is in ALL_NODE_TYPES
                 x[i, generic_idx] = 1.0
            # else: x remains all zeros for this node, which is also a valid representation

    # Edge index (edge_index)
    source_nodes_indices = []
    target_nodes_indices = []
    for edge in graph.edges.values():
        source_idx = node_id_to_idx.get(edge.source_node_id)
        target_idx = node_id_to_idx.get(edge.target_node_id)

        if source_idx is not None and target_idx is not None:
            source_nodes_indices.append(source_idx)
            target_nodes_indices.append(target_idx)
        # else:
            # This case means an edge points to/from a node not in node_ids_sorted,
            # which shouldn't happen if graph is consistent.
            # print(f"Warning: Edge '{edge.id}' references a source/target node ID not found in the sorted node ID list. Skipping edge.")

    edge_index = torch.tensor([source_nodes_indices, target_nodes_indices], dtype=torch.long)

    # Create PyG Data object
    pyg_data = Data(x=x, edge_index=edge_index)
    # pyg_data.num_nodes = len(node_ids_sorted) # PyG infers num_nodes from x if x is present

    # Optionally, store original graph ID or other graph-level properties
    # setattr(pyg_data, 'original_graph_id', graph.id)
    # for key, value in graph.properties.items():
    #     if isinstance(value, (int, float, str, list, torch.Tensor)): # Check for valid Data attributes
    #         try:
    #             setattr(pyg_data, key, value)
    #         except TypeError: # Some types might not be directly assignable
    #             print(f"Warning: Could not assign graph property '{key}' to PyG Data object.")

    return pyg_data

if __name__ == '__main__':
    # Basic test for the converter
    print("--- gnn_utils.py direct execution test ---")
    print(f"NODE_FEATURE_DIM based on ALL_NODE_TYPES: {NODE_FEATURE_DIM}")
    print(f"First few node types in ALL_NODE_TYPES: {ALL_NODE_TYPES[:5]}")
    print(f"Node type 'FunctionDef' maps to index: {NODE_TYPE_TO_INDEX.get('FunctionDef')}")

    # Create a dummy NetworkGraph
    g = NetworkGraph("dummy_graph")
    n1 = ArchitecturalNode("n1", "FunctionDef")
    n2 = ArchitecturalNode("n2", "IfStatement")
    n3 = ArchitecturalNode("n3", "instruction") # Assembly type
    g.add_node(n1)
    g.add_node(n2)
    g.add_node(n3)
    g.add_edge(ArchitecturalEdge("n1", "n2", "e1"))
    g.add_edge(ArchitecturalEdge("n2", "n3", "e2"))
    g.add_edge(ArchitecturalEdge("n1", "n3", "e3")) # Another edge

    print(f"\nOriginal NetworkGraph: {g.id}, Nodes: {len(g.nodes)}, Edges: {len(g.edges)}")

    pyg_graph_data = network_graph_to_pyg_data(g)

    if pyg_graph_data:
        print("\nConverted PyG Data object:")
        print(pyg_graph_data)
        print(f"  Node features (x) shape: {pyg_graph_data.x.shape}")
        print(f"  Edge index (edge_index) shape: {pyg_graph_data.edge_index.shape}")
        print(f"  Number of nodes in PyG data: {pyg_graph_data.num_nodes}")

        # Verify one-hot encoding for a known node
        n1_idx = sorted(list(g.nodes.keys())).index("n1") # Get the integer index of n1
        n1_type_idx_expected = NODE_TYPE_TO_INDEX.get("FunctionDef")
        if n1_type_idx_expected is not None:
            print(f"  Node n1 (index {n1_idx}) feature vector (first few): {pyg_graph_data.x[n1_idx, :5].tolist()}...")
            print(f"  Expected one-hot index for FunctionDef: {n1_type_idx_expected}")
            assert pyg_graph_data.x[n1_idx, n1_type_idx_expected].item() == 1.0, "One-hot encoding mismatch for n1"
            assert pyg_graph_data.x[n1_idx].sum().item() == 1.0, "Node feature vector for n1 is not one-hot"

        n3_idx = sorted(list(g.nodes.keys())).index("n3")
        n3_type_idx_expected = NODE_TYPE_TO_INDEX.get("instruction")
        if n3_type_idx_expected is not None:
             print(f"  Node n3 (index {n3_idx}) feature vector (relevant part): ...{pyg_graph_data.x[n3_idx, n3_type_idx_expected-1:n3_type_idx_expected+2].tolist()}...")
             print(f"  Expected one-hot index for instruction: {n3_type_idx_expected}")
             assert pyg_graph_data.x[n3_idx, n3_type_idx_expected].item() == 1.0, "One-hot encoding mismatch for n3"
             assert pyg_graph_data.x[n3_idx].sum().item() == 1.0, "Node feature vector for n3 is not one-hot"

    else:
        print("Conversion returned None.")

    print("\n--- Test with empty graph ---")
    empty_g = NetworkGraph("empty_g")
    empty_pyg_data = network_graph_to_pyg_data(empty_g)
    if empty_pyg_data:
        print(empty_pyg_data)
        assert empty_pyg_data.num_nodes == 0
        assert empty_pyg_data.x.shape == (0, NODE_FEATURE_DIM)
        assert empty_pyg_data.edge_index.shape == (2,0)
    else:
        print("Conversion of empty graph returned None (should return empty Data obj).")


    print("\n--- gnn_utils.py direct execution test complete ---")
